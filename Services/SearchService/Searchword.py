from Services.WordEmbeddingServices.WordEmbeddingModelData import WordEmbeddingModelData
from Services.FilesManagmentService.FilesServices import get_document_from_db
from Services.TextProcessingService.TextProcessingServices import cleanTokenizeText

class SearchEngineWord2VecOnly:

    def __init__(self, dataset_name):
        self.results = []
        self.dataset_name = dataset_name
        self.corrected_query = None

        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Word2Vec ÙÙ‚Ø·
        self.word2vec_model = WordEmbeddingModelData(dataset_name)
        
      # --------- Ø£Ø¯Ø§Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© ---------
    def expand_query_with_similars(self, query, topn=3):
        tokens = cleanTokenizeText(query)               # tokenize
        expanded_tokens = list(tokens)                  # Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©

        for tok in tokens:
            similars = self.word2vec_model.search_similar_words(tok, topn=topn)
            # ÙƒÙ„ Ø¹Ù†ØµØ± ÙÙŠ similars Ù‡Ùˆ (word, score)
            expanded_tokens.extend([w for w, _ in similars])

        return " ".join(expanded_tokens)
    
    def search(self, query, top_k=20, expand=True, topn_similar=3):
        """
        expand=True  : ÙŠÙØ¹Ù‘Ù„ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        topn_similar : Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        """
        try:
            # 1) ÙˆØ³Ù‘ÙØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
            expanded_query = self.expand_query_with_similars(query, topn=topn_similar) if expand else query
            print(f"ğŸ“ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…ÙˆØ³Ù‘Ø¹: [{expanded_query}]")

            # 2) Ø­ÙˆÙ‘ÙÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ Ù…ØªÙ‘Ø¬Ù‡
            query_vec = self.word2vec_model.convertQueryToVector(expanded_query)

            # 3) Ø§Ø­Ø³Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
            sims = self.word2vec_model.getSimilarityScores(query_vec)
            top_indices = sims.argsort()[::-1][:top_k]

            # 4) Ø¬Ù‡Ù‘Ø² Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¹Ø±Ø¶
            results = []
            for idx in top_indices:
                doc_name = self.word2vec_model.get_document_name(idx)   # ÙŠØ­ØªØ§Ø¬ Ø¯Ø§Ù„Ø© ÙƒÙ…Ø§ Ø£ÙˆØ¶Ø­Ù†Ø§
                doc     = get_document_from_db(self.dataset_name, doc_name)

                if doc:        # Ù‚Ø¯ ÙŠÙƒÙˆÙ† None Ù„Ùˆ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ØºÙŠØ± Ù…Ø®Ø²Ù‘ÙÙ†Ø©
                    results.append({
                        "doc_id":   idx,
                        "doc_name": doc_name,
                        "score":    float(sims[idx]),
                        "content":  doc.get("text", "")
                    })

            return results

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []