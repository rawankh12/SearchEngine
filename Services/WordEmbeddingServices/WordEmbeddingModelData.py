import os
import joblib
import numpy as np
from gensim.models import Word2Vec
from Services.TextProcessingService.TextProcessingServices import cleanTokenizeText
from Services.FilesManagmentService.FilesServices import *
from Services.DatabaseService.DatabaseServices import get_all_documents_contents, get_all_document_ids
from sklearn.metrics.pairwise import cosine_similarity


class WordEmbeddingModelData:

    def __init__(self, dataset_name, firstTime=False):
        self.dataset_name = dataset_name
        self.model = None
        self.doc_vectors = None

        if not firstTime:
            try:
                self.model = getObjectFrom_Joblib_File(dataset_name, "EmbeddingModel")
                print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Word2Vec Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)} - Ø³ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ø§Ù„Ø¢Ù†")
                self.train_model()

            try:
                self.doc_vectors = getObjectFrom_Joblib_File(dataset_name, "doc_vectors")
                print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
            except:
                print("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")


    def train_model(self):
        print('ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§...')
        raw_documents = get_all_documents_contents(self.dataset_name)
        documents = []

        for content in raw_documents:
            try:
                tokens = cleanTokenizeText(content)
                documents.append(tokens)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³ØªÙ†Ø¯: {str(e)}")

        if not documents:
            raise ValueError("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨")

        self.model = Word2Vec(
            documents,
            vector_size=100,
            window=5,
            min_count=3,
            workers=4
        )

        saveObjectTo_Joblib_File(self.model, self.dataset_name, 'EmbeddingModel.joblib')
        print("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­ÙØ¸Ù‡ Ø¨Ù†Ø¬Ø§Ø­")

    def buildDocumentVectors(self):
        print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
        raw_documents = get_all_documents_contents(self.dataset_name)
        doc_vectors = []

        for i, content in enumerate(raw_documents):
            try:
                tokens = cleanTokenizeText(content)
                embeddings = [self.model.wv[token] for token in tokens if token in self.model.wv]

                if embeddings:
                    doc_vector = np.mean(embeddings, axis=0)
                else:
                    doc_vector = np.zeros(self.model.vector_size)

                doc_vectors.append(doc_vector)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© {i}: {str(e)}")
                doc_vectors.append(np.zeros(self.model.vector_size))

        self.doc_vectors = np.array(doc_vectors)
        saveObjectTo_Joblib_File(self.doc_vectors, self.dataset_name, 'doc_vectors.joblib')
        print("âœ… ØªÙ… Ø­ÙØ¸ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")

    def convertQueryToVector(self, query):
        tokens = cleanTokenizeText(query)
        embeddings = [self.model.wv[token] for token in tokens if token in self.model.wv]

        if embeddings:
            return np.mean(embeddings, axis=0)
        else:
            return np.zeros(self.model.vector_size)

    def getSimilarityScores(self, query_vector):
        if self.doc_vectors is None:
            raise ValueError("âŒ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ØºÙŠØ± Ù…Ø­Ù…Ù‘Ù„Ø©")
        return cosine_similarity([query_vector], self.doc_vectors).flatten()

    def search_similar_words(self, word, topn=5):
        if not self.model:
            self.train_model()

        try:
            return self.model.wv.most_similar(word, topn=topn)
        except KeyError:
            print(f"âš ï¸ Ø§Ù„ÙƒÙ„Ù…Ø© '{word}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª")
            return []
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}")
            return []

    def get_document_name(self, index):
        try:
            all_ids = get_all_document_ids(self.dataset_name)
            return all_ids[index]
        except IndexError:
            print(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§Ø³Ù… ÙˆØ«ÙŠÙ‚Ø© Ù„Ù„ÙÙ‡Ø±Ø³: {index}")
            return None
