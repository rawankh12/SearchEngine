import json
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from Services.FilesManagmentService.FilesServices import *
from Services.TextProcessingService.TextProcessingServices import * 
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import word_tokenize
import numpy as np
from Services.DatabaseService.DatabaseServices import checkFileExist, get_all_document_ids, get_all_documents_contents 


class VectorSpaceModel:
    def __init__(self , datasetName , firstTime = False ):
        self.dataset_name = datasetName
        if not firstTime:
            self.vector_space_model = getObjectFrom_Joblib_File(datasetName, 'doc_vectors')
            self.document_map = getObjectFrom_Joblib_File(datasetName, 'docs_map')
            self.vectorizer = getObjectFrom_Joblib_File(datasetName, 'Model')
            # self.inverted_index = getObjectFrom_Joblib_File(datasetName, 'inverted_index')  

   
    def buildeVectorSpaceModel(self):
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§
     if all([
        checkFileExist(self.dataset_name, "doc_vectors.joblib"),
        checkFileExist(self.dataset_name, "Model.joblib"),
        checkFileExist(self.dataset_name, "docs_map.joblib")
     ]):
        print("âœ… Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨.")
        return

     print("\nðŸ“¡ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

     raw_documents = get_all_documents_contents(self.dataset_name)
     document_ids = get_all_document_ids(self.dataset_name)

     documents_texts = []
     valid_doc_ids = []

     for i, content in enumerate(raw_documents):
        cleaned = cleanTextAlgorithm(content)
        if cleaned and cleaned.strip():
            documents_texts.append(cleaned)
            valid_doc_ids.append(document_ids[i])
        else:
            print(f"âš ï¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø±Ù‚Ù… {i} ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡ (Ø§Ù„Ù†Øµ ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©)")

     if not documents_texts:
        print("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØµØ§Ù„Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.")
        return

     print(f"\nâœ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„Ø©: {len(documents_texts)}")
     print("\nðŸ” Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ù…Ø³ØªÙ†Ø¯Ø§Øª:")
     for i in range(min(5, len(documents_texts))):
        print(f"\n--- Ù…Ø³ØªÙ†Ø¯ {i+1} ---\n{documents_texts[i]}")

     # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
     vectorizer = TfidfVectorizer(
        tokenizer=word_tokenize,
        max_df=0.75,
        preprocessor=cleanTextAlgorithm  
     )

     print("\nâš™ï¸ Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ TF-IDF...")
     vectorizer.fit(documents_texts)
     self.vector_space_model = vectorizer.transform(documents_texts)
     self.vectorizer = vectorizer
     self.document_map = valid_doc_ids

     # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
     saveObjectTo_Joblib_File(self.vector_space_model, self.dataset_name, "doc_vectorsvsm.joblib")
     saveObjectTo_Joblib_File(self.vectorizer, self.dataset_name, "Model.joblib")
     saveObjectTo_Joblib_File(self.document_map, self.dataset_name, "docs_map.joblib")

     print("\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ ÙˆØ®Ø±ÙŠØ·Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù†Ø¬Ø§Ø­.")
    
    
        # âœ³ï¸ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³
        # self.inverted_index = self.build_inverted_index(documents_texts)
        # saveObjectTo_Joblib_File(self.inverted_index, self.dataset_name, "inverted_index")

        # Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ø¨ØµÙŠØºØ© JSON Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        # json_path = getprojectPath() + f"Services/FilesManagmentService/storge/Datasets/{self.dataset_name}/inverted_index.json"
        # with open(json_path, 'w', encoding='utf-8') as f:
        #   json.dump(self.inverted_index, f, ensure_ascii=False, indent=4)

  
    # def build_inverted_index(self, documents):
    #     inverted_index = {}
    #     for idx, doc in enumerate(documents):
    #         tokens = word_tokenize(cleanTextAlgorithm(doc))
    #         for token in set(tokens):  # Ø§Ø³ØªØ®Ø¯Ø§Ù… set Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    #             if token not in inverted_index:
    #                 inverted_index[token] = []
    #             inverted_index[token].append(idx)
    #     return inverted_index

    # def get_inverted_index(self):
    #     return self.inverted_index
    
    def buildDocumentsMap(self):
        print("please wait...")
        docs_path = getprojectPath() + "Services/FilesManagmentService/storge/Datasets/" + self.dataset_name + "/docs"
        document_map = [get_document_Id_Joblib_File(self.dataset_name, filename) for filename in os.listdir(docs_path)]
        saveObjectTo_Joblib_File(document_map, self.dataset_name, "docs_map")


    def printVectorSpaceModel(self):
        df = pd.DataFrame(self.vector_space_model.toarray(), columns=self.vectorizer.get_feature_names_out())
        print(df)

    def getWords(self):
        return self.vectorizer.get_feature_names_out()

    def convertQueryToVector(self , query):
        return self.vectorizer.transform([query])

    def getSimilarityDocuments(self , queryVector , top_k):
        similarity = cosine_similarity(queryVector , self.vector_space_model)
        similarity = similarity.flatten()
        sorted_indices = similarity.argsort()[::-1]
        top_documents = sorted_indices[:top_k]
        return [index for index in top_documents if similarity[index] > 0.00009]

    def getDocumentVectors(self, doc_id=None):
        if doc_id is not None:
            if doc_id in self.vector_space_model:
                return self.vector_space_model[doc_id]
            else:
                return None
        return self.vector_space_model

    def get_document_name(self, index):
        return self.document_map[index]

    def setVectorizer(self , vectorizer):
        self.vectorizer = vectorizer

    def setDocumentMap(self , document_map):
        self.document_map = document_map  

    def setDocumentVectors(self , doc_vectors):
        self.vector_space_model = doc_vectors

    def getDocumentMap(self):
        return self.document_map 

    def get_vectorizer(self):
        return self.vectorizer
    